# TODO 深度学习与python——8  深度学习

# TODO 提高识别精度
#  集成学习、学习率衰减、Data Augmentation(数据扩充)等
#  Data Augmentation基于算法“人为地”扩充输入图像(训练图像)。
#  具体地说对于输入图像,通过施加旋转、垂直或水平方向上的移动等微小变化,增加图像的数量。
#  其他各种方法扩充图像,比如裁剪图像的“crop处理”、将图像左右翻转的“flip处理”等。
#  对于一般的图像,施加亮度等外观上的变化、放大缩小等尺度上的变化也是有效的。

# TODO 加深层数
#  1.减少网络的参数数量
#  一次5 × 5的卷积运算的区域可以由两次3 × 3的卷积运算抵充。
#  前者的参数数量25(5 × 5),后者一共是18(2 × 3 × 3)
#  叠加小型滤波器来加深网络的好处是可以减少参数的数量,扩大感
#  受野(receptive field,给神经元施加变化的某个局部空间区域)。并且,
#  通过叠加层,将 ReLU等激活函数夹在卷积层的中间,进一步提高
#  了网络的表现力。这是因为向网络添加了基于激活函数的“非线性”
#  表现力,通过非线性函数的叠加,可以表现更加复杂的东西。
#  2.使学习更加高效
#  通过加深层,可以分层次地传递信息,可以分层次地分解需要学习的问题,
#  可以将各层要学习的问题分解成容易解决的简单问题,从而可以进行高效的学习

# TODO VGG
#  VGG是由卷积层和池化层构成的基础的CNN。不过,如图8-9所示,
#  它的特点在于将有权重的层(卷积层或者全连接层)叠加至16层(或者19层),
#  具备了深度(根据层的深度,有时也称为“VGG16”或“VGG19”)。
#  VGG中需要注意的地方是,基于3×3的小型滤波器的卷积层的运算是
#  连续进行的。如图8-9所示,重复进行“卷积层重叠2次到4次,再通过池化
#  层将大小减半”的处理,最后经由全连接层输出结果。

# TODO　GoogLeNet
#  和之前介绍的CNN结构相同。不过,GoogLeNet的特征是,网络不仅
#  在纵向上有深度,在横向上也有深度(广度)。
#  GoogLeNet在横向上有“宽度”,这称为“Inception结构”,以图8-11所
#  示的结构为基础。
#  如图8-11所示,Inception结构使用了多个大小不同的滤波器(和池化),
#  最后再合并它们的结果。GoogLeNet的特征就是将这个Inception结构用作
#  一个构件(构成元素)。此外,在GoogLeNet中,很多地方都使用了大小为
#  1 × 1的滤波器的卷积层。这个1 × 1的卷积运算通过在通道方向上减小大小,
#  有助于减少参数和实现高速化处理(具体请参考原始论文[23])。

# TODO ResNet
#  它的特征在于具有比以前的网络更深的结构。
#  为了解决这类问题(过度加深层,导致最终性能不佳),导入了“快捷结构”。
#  可以随着层的加深而不断提高性能了(当然,层的加深也是有限度的)
#  因为快捷结构只是原封不动地传递输入数据，所以反向传播时会将
#  来自上游的梯度原封不动地传向下游。这里的重点是不对来自上游
#  的梯度进行任何处理，将其原封不动地传向下游。因此，基于快捷
#  结构，不用担心梯度会变小（或变大），能够向前一层传递“有意义
#  的梯度”。通过这个快捷结构，之前因为加深层而导致的梯度变小的
#  梯度消失问题就有望得到缓解

# TODO 深度学习的高速化
#  1.基于GPU的高速化
#  2.分布式学习
#  关于分布式学习，“如何进行分布式计算”是一个非常难的课题。它包
#  含了机器间的通信、数据的同步等多个无法轻易解决的问题。可以将这些难
#  题都交给TensorFlow等优秀的框架。
#  关于分布式学习的技术性内容，请参考TensorFlow的技术论文（白皮书）等。
#  3.运算精度的位数缩减
#  深度学习并不那么需要数值精度的位数
#  这个性质是基于神经网络的健壮性(比如，即便输入图像附有
#  一些小的噪声，输出结果也仍然保持不变。)而产生的。
#  特别是在面向嵌入式应用程序中使用深度学习时，位数缩减非常重要

